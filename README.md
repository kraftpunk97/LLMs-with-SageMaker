# LLMs with SageMaker

In this project, we will deploy Llama-2 on AWS Sagemaker and perform prompt engineering for an AI Vacation Manager


## MLOps on Sagemaker to deploy Llama-2
1.  Install Sagemaker on this Notebook Instance.

2.  Create Sagemaker session, assign buckets and print session details.

3.  Get Hugging Face model (Llama-2) from Deep Learning Containers (DLC).

4.  Set necessary configs to use Hugging Face model.

5. Deploy the Llama-2-7b-chat-hf model.

6. Give Prompt to Llama-2 to get outputs.



## Prompt Engineering

### Approaches

We will see the various approaches used in Prompt Engineering and compare the output for each of them.

* __Chain of Thoughts (CoT)__: A sequential reasoning approach where the solution to a complex problem is derived through a series of logical steps.

* __Tree of Thoughts (ToT)__: A hierarchical reasoning method that organizes thoughts in a tree-like structure to explore different facets or solutions of a problem.

* __Autoregressive Template (ART)__: A template-driven approach that generates text by building on previous outputs in a stepwise manner, enhancing coherence and contextuality.

* __Reasoning-Aided Conversation Template (ReACT)__: Combines conversational context with explicit reasoning steps to produce responses that are both engaging and logically structured.

* __Self-Consistency (SC)__: A method focused on generating multiple answers and then refining or choosing between them based on consistency and coherence criteria.

* __Retrieval-Augmented Generation (RAG)__: Integrates external knowledge or data into the generation process to inform and enhance the quality and relevance of the output.

### Parameters

The parameters specified in the Llama-2 payload control the behavior of the model's text generation process. Each parameter influences how the output is generated, offering a way to customize the results according to specific needs. Here's a detailed explanation of each parameter:

* `do_sample`: This parameter determines whether the model should generate text by sampling from the distribution of possible next tokens (when set to True) or by always choosing the most likely next token (when set to False). Sampling can introduce variability and creativity in the responses, making the text more diverse and less deterministic.

* `top_p` (also known as nucleus sampling): This is a threshold parameter used to control the diversity of the generated text. It specifies a cumulative probability cutoff to select a subset of the vocabulary tokens for sampling. For example, if top_p is set to 0.6, the model will only consider the smallest set of tokens whose cumulative probability exceeds 60% for sampling. This helps in focusing the sampling process on more likely tokens while still allowing for some creativity and variation in the responses.

* `temperature`: This parameter controls the randomness of the output generation. A higher temperature results in more random outputs, while a lower temperature makes the model's outputs more deterministic and conservative. For instance, a temperature of 0.8 moderates the balance between randomness and determinism, aiming to produce outputs that are neither too random nor too predictable.

* `top_k`: This parameter limits the number of highest probability vocabulary tokens to consider for each step in the text generation process. Only the top k tokens are considered for sampling, which helps to filter out less likely tokens and can prevent the model from generating implausible text. A value of 50 means that only the top 50 tokens according to their probability will be considered at each step.

* `max_new_tokens`: This specifies the maximum number of new tokens to be generated by the model. In this context, a token can be a word or part of a word, so this parameter effectively controls the maximum length of the generated text. Setting it to 512 limits the output to 512 tokens, which helps in managing the output size and ensuring responses are not overly long.

* `repetition_penalty`: This parameter is used to discourage the model from repeating the same words or phrases. A penalty greater than 1.0 decreases the likelihood of tokens that have already appeared in the output, encouraging the model to generate more diverse and less repetitive text. A value of 1.03 applies a slight penalty to repetitions, aiming to maintain a balance between coherence and repetition avoidance.

Together, these parameters offer a nuanced control over the text generation process, allowing users to fine-tune the model's behavior to match specific requirements for creativity, diversity, length, and coherence of the generated text.


### Testing

For our testing, we first try out various approaches to Prompt Engineering without changing the aforementioned parameters, to test how the approaches perform against one another. We then tweak the above-mentioned parameters to see their influence on the results.

## Final Conclusion

For the task fo vacation planning, when inputting basic promts of various type, the best permance was for: ART, Self Consistency and RAG.

For the task fo vacation planning, when inputting basic promts of various type, the better outputs were generated was when: increase `top_k`, increase `top_p` and decrease `temperature`.

